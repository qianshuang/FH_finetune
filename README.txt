# 微调训练
deepspeed --include localhost:1 fine-tune.py \
--report_to "none" \  # 或者"tensorboard"
--data_path "data/ft_baichuan_data.json" \
--model_name_or_path "/opt/qs/aliendao/dataroot/models/baichuan-inc/Baichuan2-13B-Chat" \
--output_dir "/opt/qs/aliendao/dataroot/models/finetune/Baichuan2-13B-Chat" \
--model_max_length 4096 \
--num_train_epochs 2 \
--per_device_train_batch_size 4 \
--gradient_accumulation_steps 2 \
--save_strategy epoch \
--learning_rate 2e-5 \
--lr_scheduler_type constant \
--adam_beta1 0.9 \
--adam_beta2 0.98 \
--adam_epsilon 1e-8 \
--max_grad_norm 1.0 \
--weight_decay 1e-4 \
--warmup_ratio 0.0 \
--logging_steps 1 \
--gradient_checkpointing True \
--deepspeed ds_config.json \
--bf16 True \
--tf32 True \  # TF32为用于训练和推理的AI应用程序提供了巨大的开箱即用性能提升，零代码更改，同时能够保持FP32级别的准确性。
--use_lora True
